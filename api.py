from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForImageTextToText, AutoProcessor
import time

app = Flask(__name__)

print("=" * 60)
print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ TranslateGemma-4B...")
print("=" * 60)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

if torch.cuda.is_available():
    print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 3080
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    print("‚úÖ TF32 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_id = "google/translategemma-4b-it"
print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_id}")

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    low_cpu_mem_usage=True,
)

print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

# –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    print(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ VRAM: {allocated:.2f} GB")

print("=" * 60)
print("üåê API —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
print("=" * 60)

@app.route('/', methods=['GET'])
def home():
    return jsonify({
        "service": "TranslateGemma API",
        "model": "google/translategemma-4b-it",
        "status": "running",
        "endpoints": {
            "translate": "POST /translate - –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞",
            "translate_image": "POST /translate-image - –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
            "health": "GET /health - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è",
            "languages": "GET /languages - –°–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —è–∑—ã–∫–æ–≤"
        }
    })

@app.route('/health', methods=['GET'])
def health():
    gpu_info = {}
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_info = {
            "cuda_available": True,
            "gpu_name": torch.cuda.get_device_name(0),
            "vram_used_gb": round(allocated, 2),
            "vram_total_gb": round(total, 2),
            "vram_free_gb": round(total - allocated, 2)
        }
    
    return jsonify({
        "status": "healthy",
        "model": "translategemma-4b",
        "device": device,
        "gpu": gpu_info
    })

@app.route('/languages', methods=['GET'])
def languages():
    # 55 —è–∑—ã–∫–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö TranslateGemma
    supported_languages = {
        "en": "English",
        "de": "German", 
        "fr": "French",
        "es": "Spanish",
        "it": "Italian",
        "pt": "Portuguese",
        "ru": "Russian",
        "zh": "Chinese",
        "ja": "Japanese",
        "ko": "Korean",
        "ar": "Arabic",
        "hi": "Hindi",
        "cs": "Czech",
        "pl": "Polish",
        "nl": "Dutch",
        "sv": "Swedish",
        "da": "Danish",
        "no": "Norwegian",
        "fi": "Finnish",
        "tr": "Turkish",
        "el": "Greek",
        "he": "Hebrew",
        "th": "Thai",
        "vi": "Vietnamese",
        "id": "Indonesian",
        # –ò –µ—â–µ 30+ —è–∑—ã–∫–æ–≤...
    }
    return jsonify({
        "supported_languages": supported_languages,
        "total": 55,
        "note": "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–∞–∫–∂–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, en-US, de-DE)"
    })

@app.route('/translate', methods=['POST'])
def translate():
    try:
        data = request.json
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not data.get("text"):
            return jsonify({"error": "–ü–æ–ª–µ 'text' –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ"}), 400
        
        source_lang = data.get("source_lang", "en")
        target_lang = data.get("target_lang", "ru")
        text = data.get("text")
        
        print(f"üîÑ –ü–µ—Ä–µ–≤–æ–¥: {source_lang} ‚Üí {target_lang}")
        print(f"üìù –¢–µ–∫—Å—Ç: {text[:100]}...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "source_lang_code": source_lang,
                        "target_lang_code": target_lang,
                        "text": text,
                    }
                ],
            }
        ]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–∞
        start_time = time.time()
        
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(device, dtype=torch.bfloat16)
        
        input_len = len(inputs['input_ids'][0])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=data.get("max_tokens", 200),
                do_sample=False,
                temperature=0.1,
                use_cache=True,
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        generation = generation[0][input_len:]
        translation = processor.decode(generation, skip_special_tokens=True)
        
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed_time:.2f}—Å")
        print(f"üì§ –†–µ–∑—É–ª—å—Ç–∞—Ç: {translation[:100]}...")
        
        return jsonify({
            "translation": translation,
            "source_lang": source_lang,
            "target_lang": target_lang,
            "model": "translategemma-4b",
            "processing_time_seconds": round(elapsed_time, 2)
        })
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/translate-image', methods=['POST'])
def translate_image():
    try:
        data = request.json
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not data.get("image_url"):
            return jsonify({"error": "–ü–æ–ª–µ 'image_url' –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ"}), 400
        
        source_lang = data.get("source_lang", "en")
        target_lang = data.get("target_lang", "ru")
        image_url = data.get("image_url")
        
        print(f"üñºÔ∏è  –ü–µ—Ä–µ–≤–æ–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {source_lang} ‚Üí {target_lang}")
        print(f"üîó URL: {image_url}")
        
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source_lang_code": source_lang,
                        "target_lang_code": target_lang,
                        "url": image_url,
                    }
                ],
            }
        ]
        
        start_time = time.time()
        
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(device, dtype=torch.bfloat16)
        
        input_len = len(inputs['input_ids'][0])
        
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=data.get("max_tokens", 200),
                do_sample=False,
                use_cache=True
            )
        
        generation = generation[0][input_len:]
        translation = processor.decode(generation, skip_special_tokens=True)
        
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed_time:.2f}—Å")
        print(f"üì§ –†–µ–∑—É–ª—å—Ç–∞—Ç: {translation[:100]}...")
        
        return jsonify({
            "translation": translation,
            "source_lang": source_lang,
            "target_lang": target_lang,
            "model": "translategemma-4b",
            "processing_time_seconds": round(elapsed_time, 2)
        })
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)

