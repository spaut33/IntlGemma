# TranslateGemma Configuration

# HuggingFace Hub Settings
HF_HOME=/root/.cache/huggingface
HF_TOKEN=  # Optional: Your HuggingFace token for private models

# Model Settings
MODEL_ID=google/translategemma-4b-it
TORCH_DTYPE=bfloat16
DEVICE=auto  # auto, cuda, cpu

# GPU Optimizations
ENABLE_TF32=true

# API Settings
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=false

# Translation Defaults
DEFAULT_SOURCE_LANG=en
DEFAULT_MAX_TOKENS=200
DEFAULT_BATCH_SIZE=20
